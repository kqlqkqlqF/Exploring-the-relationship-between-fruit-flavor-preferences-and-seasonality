---
title: "Modeling Price Fluctuations of Seasonal Fruits in Food Products: A Predictive Analysis"
subtitle: "Trump's Narrow Victory Over Harris by Less Than One Percent of the Supporting Rate"
author: 
  - Yiyi Feng
thanks: "Code and data are available at: https://github.com/kqlqkqlqF/Insights-and-Predictions-for-the-U.S.-Election.git."
date: today
date-format: long
abstract: "This study presents a predictive model for the 2024 U.S. Presidential Election, focusing on the race between Donald Trump and Kamala Harris. Our model forecasts a narrow victory for Trump, estimating his average support at 44.51% compared to Harris's 43.86%, with leads of Trump in swing states. The analysis shows that state and recency are important for understanding voter support trends, reflecting the electoral system's winner-takes-all nature. This research allows electoral forecasting by demonstrating how localized support influences national outcomes and shows the need for improved polling methodologies."
format:
  pdf:
    toc: true
    number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

#### Preamble ####
# Purpose: Help constructing the "Predictive Modeling for Forecasting the 2024 US Presidential Election" paper

# Author: Bo Tang, Yiyi Feng, Mingjing Zhan
# Date: 1 November 2024
# Contact: qinghe.tang@mail.utoronto.ca, yiyi.feng@mail.utoronto.ca, mingjin.zhan@mail.utoronto.ca

####Workspace setup ####

library(dplyr)
library(tibble)
library(here)
library(modelsummary)
library(tidyverse)
library(sf)
library(arrow)
library(ggplot2)
library(janitor)
library(purrr)
library(knitr)
library(kableExtra)
library(usmap)
library(broom)
library(caret)
library(randomForest)
library(rstanarm)

model_data <- read_parquet(here::here("data/02-analysis_data/combined_model_data.parquet"))
rain_data <- read_parquet(here::here("data/02-analysis_data/average_rain_data.parquet"))

```


# Introduction

The upcoming U.S. Presidential Election marks an important point in the nation’s political landscape, shaped by public opinion, social and economic factors, and the complexities of the electoral process. With Kamala Harris and Donald Trump competing for the presidency, accurately predicting the outcome is increasingly important. Polls not only reflect voter opinion but also influence campaign strategies and media coverage. However, challenges like sampling biases, inconsistent methods, and the gap between the popular vote and the electoral vote emphasize the need for an improved forecasting model. This paper aims to develop a predictive framework that uses national polling data and examines state-level dynamics, especially in key swing states that often decide election results.

Our main focus is the probability of Donald Trump winning the 2024 U.S. presidential election, represented by voter support rates. To estimate support for both Trump and Harris, we developed linear models that account for factors such as candidate identity, poll recency, state, sample size, poll score, and poll quality, along with interactions among these variables. By identifying the optimal model, we aim to determine how these factors and their combinations influence expected support, providing insights into each candidate's chances across different regions and polling conditions. This approach models support rates rather than direct winning probabilities, allowing for a nuanced prediction that reflects variations by candidate, state, and recency.

Our model predicts that Donald Trump will win by a narrow margin, with an average support of 44.51% compared to Kamala Harris's 43.86%. Trump leads in six out of seven key swing states, suggesting that this localized support could enhance his overall chances despite only a slight national lead. Among the predictor variables analyzed, state and recency are the most significant indicators of support trends, reflecting the "winner-takes-all" nature of the U.S. electoral system and the increasing accuracy of polling data as Election Day approaches.

The remainder of this paper is structured as follows: [@sec-data] provides an overview of the dataset, details of the parameters, outcome and predictor variables, and the packages used during processing. [@sec-model] explains the modeling approach, and best model selection, justifying the choice of predictors and outlining the methods used to forecast support for Trump and Harris. [@sec-results] presents the findings, including a summary of the predicted support rate for Trump, a comparison of the predicted support rates for Trump and Harris, and a breakdown of their support rates in each state. In [@sec-discussion], we discuss the implications of these results, the limitations of our analysis, and potential avenues for future research. Additional methodological details and diagnostics are included in the appendix.



# Data {#sec-data}

## Overview

In this analysis, we used R [@citeR] to investigate polling data on public sentiment leading up to the election. Our dataset, sourced from FiveThirtyEight [@fivethirtyeight2024], provides a detailed snapshot of shifting public opinion over time. We examined key factors influencing support percentages, including poll timing, pollster characteristics, and state-specific trends.

Several R packages were vital for our data manipulation, modeling, and visualization efforts. The dplyr package provided efficient tools for data transformation and summarization [@dplyr], while modelsummary enhanced the presentation of model outputs in a clear and organized manner [@modelsummary]. We used sf for handling spatial data, enabling analysis of state-level dynamics in the election [@sf]. purrr streamlined functional programming, allowing for the application of functions across data structures [@purrr]. kableExtra created customizable tables to improve our data presentation [@kableExtra], and usmap facilitated mapping of electoral data across states [@usmap]. The broom package converted complex model outputs into tidy data frames for easier analysis [@broom]. Package caret provided a unified framework for building and evaluating machine learning models [@caret], while the randomForest package enabled the use of random forest modeling techniques for our predictive analysis [@randomForest]. Finally, testthat ensured the reliability of our analyses through code testing [@testthat]. Our workflow closely adhered to best practices, as outlined in [@tellingstories], enhancing the robustness of our predictive framework.

Our group focused on Trump’s approval ratings, aiming to ensure the credibility of the data. To achieve this, we selected only pollsters with numeric grades above 2.0, and used data collected from November 15, 2022, to October 27, 2024.

## Measurement
	
In this section, we will describe the process of converting raw poll data into a structured dataset for analysis. In this process, because this study focuses on studying the changes in Trump's support rate and predicting whether Trump can be successfully elected, all data collection and analysis will be carried out around Trump and his main opponent Harris. Raw poll data comes from actual polls conducted by various pollsters across the United States. Each pollster uses different methods, such as online panels and live phone surveys, to record whether the public supports Donald Trump. After the poll results are collected, they are aggregated into datasets, such as the dataset provided by FiveThirtyEight[@fivethirtyeight2024]. In this dataset, key factors include the start and end dates of the poll, the identity of the pollster, the state, the poll score, and the numeric grade, which is an indicator to evaluate the reliability of each poll. These parameters will be explained in detail below. This structured dataset allows us to analyze Trump's support patterns and trends over time and across states. We aim to find how these factors affect public sentiment and predict the likelihood of Trump becoming the next US president.

## Outcome Variables

### Overview of Trump's Electoral Support

[@fig-pct] illustrates the distribution of approval ratings for Trump. The majority of the approval ratings fall between 40% and 55%, forming a shape that resembles a normal distribution, with a peak around the 45% to 50% range. This suggests that, within the analyzed sample, most of the approval ratings cluster in this middle range, with relatively few instances of extremely high or low ratings.

The lower frequency of approval ratings below 30% and above 60% indicates that these extremes are relatively uncommon in the dataset. Overall, the concentration of support in this central range suggests a fairly consistent level of public support for Trump.

```{r}
#| label: fig-one
#| fig-cap: This figure shows the distribution of monthly price changes for banana and strawberry products. A higher concentration around zero indicates more stable prices, while wider distributions suggest greater variability.
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Create the plot for average price distribution
ggplot(model_data, aes(x = monthly_avg_price)) +
  geom_histogram(aes(y = ..density..),
                 bins = 30,
                 fill = "blue",
                 color = "black",
                 alpha = 0.7
  ) +
  geom_density(color = "black", fill = "salmon", alpha = 0.6) +
  labs(
    x = "Average Price",
    y = "Density"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    strip.text = element_text(size = 12), # Adjust facet label size
    axis.title = element_text(size = 14), # Adjust axis title size
    axis.text = element_text(size = 12)  # Adjust axis label size
  )


```

```{r}
#| label: fig-two
#| fig-cap: This figure shows the distribution of monthly price changes for banana and strawberry products. A higher concentration around zero indicates more stable prices, while wider distributions suggest greater variability.
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Create the plot for price distribution by month
ggplot(model_data, aes(x = as.factor(month), y = monthly_avg_price, color = monthly_avg_price)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Month",
    y = "Average Price",
    title = "Average Price Distribution by Month"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12)
  )


```

## Predictor Variables

### Summary of Predictor Variables

- **Vendor:** The U.S. state where the poll was conducted, if applicable.

- **Average Rainfall Per Month:** A numeric rating from 2.0 to 3.0 indicates each pollster’s reliability.

- **Food Category:** The total number of respondents participating in the poll.

- **Month:** A quantitative measure of the pollster’s reliability, where lower values suggest higher predictive accuracy.

### Vendor

According to [@fig-state], we observe an interesting trend: in traditionally Republican states, Trump's support is not markedly high and is even relatively low in places like Oklahoma and Tennessee. Conversely, in states typically aligned with the Democratic Party, as well as in swing states, Trump’s support is unexpectedly higher. The "national" category in the chart represents data spanning the entire country without focusing on specific states, showing Trump’s national support nearing but not reaching 50%. This suggests Trump’s appeal may be crossing traditional partisan lines, gaining unexpected traction outside Republican strongholds. Overall, his estimated national support stands at around 45%.

```{r}
#| label: fig-vendor
#| fig-cap: Overview of the Percentage Support of Trump Across Different States
#| echo: false
#| warning: false
#| message: false

# Create a bar plot for vendor distribution
ggplot(model_data, aes(x = vendor)) +
  geom_bar(fill = "blue", color = "black", alpha = 0.7) + 
  labs(
    x = "Vendor",
    y = "Count",
    title = "Vendor Distribution"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels for better readability
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )


```

### Average Rainfall Per Month

In [@fig-rain], we analyzed the relationship between numeric grade and Trump’s support rate. Each point in the chart represents a poll, with its numeric grade on the x-axis and Trump’s support rate on the y-axis. The nearly flat trend line suggests that numeric grade has no clear relationship with Trump’s support rate. However, this is a basic analysis and does not rule out the possibility that numeric grade could impact Trump’s support rate under different variable conditions.

```{r}
#| label: fig-rain
#| fig-cap: Relationship between Numeric Grade and Support Percentage of Trump
#| echo: false
#| warning: false
#| message: false

# Assuming rain_data is your dataset
ggplot(rain_data, aes(x = as.factor(month), y = avg_rainfall)) +
  geom_bar(stat = "identity", fill = "blue", color = "black", alpha = 0.7) + 
  labs(
    x = "Month",
    y = "Average Rainfall (mm)",
    title = "Average Rainfall per Month"
  ) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )


```

### Food Category

In [@fig-sample-size], we examined the relationship between sample size and Trump’s support rate. The results show a slight downward trend in Trump’s support as the sample size increases. However, since most data points are concentrated in the 0–4000 range, with fewer data points above 4000, this may not accurately reflect the true relationship between sample size and support rate.

```{r}
#| label: fig-category
#| fig-cap: Relationship between Sample Size and Support Percentage for Trump
#| echo: false
#| warning: false
#| message: false
# Convert pct column to numeric if it is not already

# Create the plot for price distribution by category
ggplot(model_data, aes(x = category, y = monthly_avg_price, color = monthly_avg_price)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Category",
    y = "Average Price",
    title = "Average Price Distribution by Category"
  ) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12)
  )



```

### Month

Analysis of [@fig-two] shows no clear proportional or inverse relationship between poll scores and Trump’s support rate. This suggests that while a higher poll score may indicate greater reliability, it does not directly translate into changes in candidate support. This also supports the data's credibility, as Trump’s support rate remains consistent regardless of pollster ratings.




# Model {#sec-model}

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

model_rf <-
  readRDS(file = here::here("models/model_rf.rds"))

model_linear_1 <-
  readRDS(file = here::here("models/model_linear_1.rds"))

model_linear_2 <-
  readRDS(file = here::here("models/model_linear_2.rds"))

```

The goal of this section is to address the inherent biases and variations present in polling data to build a robust predictive model. The key challenge lies in achieving an optimal balance between model complexity and fit, ensuring that the model accurately captures the dynamics of polling data while avoiding overfitting. To this end, we evaluated multiple model specifications to identify the one that best meets our forecasting objectives.

We chose to use "numeric grade" and "poll score" as variables instead of "pollster" because "pollster" tends to be highly subjective. People often select polling organizations that favor their preferred candidate, which can introduce bias. In contrast, "numeric grade" and "poll score" offer a more objective, quantified reflection of poll quality and bias, helping to improve accuracy and reliability in the regression analysis. Additionally, we focused on key factors such as sample size, state, and recency, gradually adding complexity to the model.

By systematically comparing model specifications that incorporate different variables, we aim to identify a model that strikes the right balance between predictive accuracy and generalizability, ultimately providing the reliable forecast results.

## Model Set-up

We aim to model the percentage of support for a candidate based on factors including candidate name, recency weight, state, sample size, poll score, and numeric grade. This model includes interaction terms to capture how combinations of these factors jointly impact the support percentage, providing a better understanding of the influences on candidate support.

\begin{align*}
\text{Price Change}_i = &\ \beta_0 + \beta_1 \cdot \text{Vendor}_i + \beta_2 \cdot \text{Category}_i + \beta_3 \cdot \text{Month}_i \\
& + \beta_4 \cdot \text{Avg Rainfall}_i + \epsilon_i
\end{align*}

Where

-   $\text{Price Change}_i$: The change in price for product $i$.
-   $\beta_0$: Intercept term, representing the predicted price change when all independent variables are 0.
-   $\beta_1$: Main effect of $\text{Vendor}_i$, capturing the influence of the vendor on the price change.
-   $\beta_2$: Main effect of $\text{Category}_i$, representing the impact of the product category (e.g., beverage, solid snack, fruit) on the price change.
-   $\beta_3$: Main effect of $\text{Month}_i$, reflecting the influence of the month of the year on price change.
-   $\beta_4$: Main effect of $\text{Avg Rainfall}_i$, capturing the influence of average rainfall on the price change.
-   $\epsilon_i$: The error term, assumed to follow a normal distribution with mean 0.


### Model Interpretation

This regression model is designed to predict voter support rates by incorporating factors and interaction terms. The model includes an intercept term, representing the baseline support rate when all other predictors are zero. Among the main effects, it includes terms for candidate identity, poll recency, and state, capturing the influence of these individual factors on support rate. For instance, candidate identity indicates how different candidates affect voter support, poll recency reflects how recent the poll is, and the state variable accounts for regional variations in support.

To capture more complex relationships, the model incorporates two-way interaction terms. These include interactions between candidate and recency, candidate and state, and recency and state. Each of these terms helps identify how one factor might alter the effect of another. For example, the interaction between candidate and recency shows how the influence of recency might differ for each candidate, while the interaction between candidate and state captures how support for different candidates varies by region. Additionally, the model includes a three-way interaction term among candidate identity, poll recency, and state, allowing it to account for combined effects that vary across candidates, states, and the timing of the poll.

The model also includes several other predictors, including sample size, poll score, and numeric grade. These predictors help account for variations in the data, with sample size ensuring that different poll sizes are properly weighted, poll score reflecting potential bias within each poll, and numeric grade indicating the reliability of each poll. Finally, the model includes an error term to capture any unexplained variation in support rate, adding robustness to its predictions. Overall, this structure allows the model to incorporate both straightforward and complex relationships, providing a reliable prediction of voter support.

## Model Justification

```{r}
#| label: tbl-model-compare
#| tbl-cap: Model Summary with Included Variables and Interactions
#| echo: false
#| warning: false
#| message: false

# Extract OOB error
oob_error <- model_rf$mse[model_rf$ntree]

# Number of trees
num_trees <- model_rf$ntree

# Extract variable importance
var_importance <- importance(model_rf)
importance_table <- as_tibble(var_importance, rownames = "Variable") %>%
  rename(
    `%IncMSE` = `%IncMSE`,
    `IncNodePurity` = `IncNodePurity`
  )

# Construct a summary table for model metrics
model_metrics <- tibble(
  Metric = c("Number of Trees", "Out-of-Bag Error (MSE)"),
  Value = c(num_trees, round(oob_error, 5))
)

# Display model metrics table
model_metrics %>%
  kable(digits = 5, escape = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "Random Forest Model Metrics" = 1)) %>%
  column_spec(1, bold = TRUE, width = "12em")

# Display variable importance table
importance_table %>%
  mutate(
    `%IncMSE` = round(`%IncMSE`, 2),
    `IncNodePurity` = round(`IncNodePurity`, 2)
  ) %>%
  kable(digits = 2, escape = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "Variable Importance in Random Forest" = 2)) %>%
  column_spec(1, bold = TRUE, width = "12em")


```

[@tbl-model-compare] summarizes the performance metrics for five models, each with progressively more variables and interactions.

Model 1, which includes only basic predictors (Sample Size, Poll Score, Numeric Grade, and State), shows limited explanatory power, with an R² of 0.0526 and a high RMSE of 5.39274, indicating poor predictive accuracy. Adding Recency Weight in Model 2 slightly improves performance, increasing R² to 0.09461, but the RMSE remains high at 5.27184, suggesting only minor gains in prediction accuracy. Model 3 further incorporates Candidate Name, leading to a modest increase in R² to 0.09644, yet with minimal impact on RMSE (5.26649), indicating limited additional explanatory value from this variable alone.

The inclusion of an interaction between Candidate Name and State in Model 4 significantly enhances the model’s fit, raising R² to 0.46417 and reducing RMSE to 4.05562. This improvement suggests that state-specific variations in candidate popularity are important for predictive accuracy. Finally, Model 5 builds upon Model 4 by adding a three-way interaction among Candidate Name, State, and Recency Weight. This final model achieves the highest R² (0.48581) and the lowest RMSE (3.97288), indicating the best fit and prediction accuracy across all models.

In conclusion, Model 5 is chosen as it captures complex interactions and provides the best balance of explanatory power and prediction accuracy, as evidenced by its highest R² and lowest RMSE.

## Optimization

Since we identified the limitations of linear models in handling interaction terms during our research, if we wish to investigate this issue further, we propose an optimized model—Random Forest. The reason we chose Random Forest over a linear model is because our data includes interaction terms. Random Forest excels at handling interactions and nonlinear relationships, as it can automatically identify and leverage these complex interactions through its decision tree structure, without requiring manual adjustments to the model structure. In contrast, linear models have limited capabilities for handling interactions, typically requiring manual specification and hard to capture nonlinear effects. Given that our data contains multiple interactions, Random Forest can more flexibly adapt to the data structure, improving prediction accuracy and model stability.

```{r}
#| include: false
#| warning: false
#| message: false


```


```{r}
#| label: tbl-rf
#| tbl-cap: "Predicted Average Supporting Percentages for Donald Trump vs. Kamala Harris by Random Forest"
#| echo: false
#| warning: false
#| message: false



```

[@tbl-rf] demonstrates the prediction results obtained through Random Forest. Compared to a linear model, this result may be more accurate; however, in the overall study, our main focus is the linear model.


# Result {#sec-results}

```{r}
#| label: tbl-trump
#| tbl-cap: "Summary Statistics of Predicted Support for Donald Trump"
#| message: false
#| echo: false
#| warning: false



```

[@tbl-trump] shows the statistical information of the predicted support rate for Trump. The average support rate is 44.51%, with a median of 44.44%. The support rate ranges from a minimum of 28% to a maximum of 67.6%. The standard deviation of the support rate is 3.98, indicating some variability in the predictions. The data is derived from 2248 polls.

```{r}
#| label: tbl-modelresults
#| tbl-cap: "Predicted Average Supporting Percentages for Donald Trump vs. Kamala Harris"
#| warning: false
#| echo: false
#| eval: true



```

[@tbl-modelresults] presents the model's average predicted supporting percentages for Donald Trump and Kamala Harris, along with their normalized percentages. According to the model's predictions, Donald Trump's average predicted supporting percentage is 44.51%, while Kamala Harris's is 43.86%. The normalized percentages adjust these predicted values to relative proportions, with Donald Trump at 50.37% and Kamala Harris at 49.63%. These results show that although Donald Trump's average predicted supporting percentage is slightly higher than Kamala Harris's, the support rates for both candidates are nearly equal, with Donald Trump holding a slight edge.

```{r}
#| label: fig-map
#| fig-cap: "Map of Projected Winner by State"
#| warning: false
#| echo: false
#| eval: true


```

[@fig-map] shows the predicted winner by state. Red indicates Trump’s predicted lead, blue indicates Harris’s lead, and gray marks states with insufficient data to predict the winner. This map visually represents regional support patterns for both candidates across the country. Trump shows strength in parts of the Midwest and South, while Harris performs better in parts of the Northeast and West. The table below [@tbl-model-state] presents the specific support rate figures.

```{r}
#| label: tbl-model-state
#| tbl-cap: "Table of Projected Winner by State"
#| echo: false
#| eval: true
#| warning: false



```

[@tbl-model-state] provides detailed data for each state’s predicted support rate and expected winner. Together with [@fig-map], it offers a better understanding of Trump and Harris’s support levels in each state.

# Discussion {#sec-discussion}

## Summery of Findings

Our model predicts a narrow victory for Donald Trump, with an average support of 44.51% compared to Kamala Harris’s 43.86%. This result aligns with the predictions from our Random Forest model in [@tbl-rf], which also suggests a slight edge for Trump. However, since a higher popular vote does not guarantee an election win, and both models indicate no significant lead, we hesitate to conclude that Trump is highly likely to win.

Looking at support across states, Trump has a clear advantage in the swing states, which strengthens his position in our prediction. Out of seven swing states: Wisconsin, Pennsylvania, North Carolina, Nevada, Michigan, Georgia, and Arizona [@swing], Trump leads six. Since swing states play an important role in the election outcome, we believe that even with a slight national lead, Trump’s support in these states could give him an advantage over Harris.

Among all predictor variables, state and the recency of poll data stand out as the most effective indicators of support trends. This finding aligns with structural aspects of the U.S. election system: most states use a “winner-takes-all” approach, where winning the majority yields all electoral votes, and many states have strong historical party preferences. Consequently, state-level support has a direct impact on the election outcome. Additionally, as Election Day nears, polling data becomes increasingly accurate, better reflecting actual voter intentions.

## Limitation

Our analysis focuses exclusively on data related to Trump, which may weaken the overall analysis. In organizing the data, we excluded poll data with low numeric grades to enhance its credibility; however, this approach resulted in a smaller sample size and reduced coverage. When examining the relationship between state and supporting percentage (PCT), we did not integrate PCT with sample size for comparison. We believe that all decisions made by the collective data should be treated equally, regardless of sample size. This perspective has led to some counterintuitive results, such as states where Trump’s party is strong giving him fewer votes. Additionally, a significant portion of our raw data lacks exact state labels, which introduces errors in analyzing the relationship between state and PCT, even after processing.

Choosing a linear regression model to predict election results has several clear drawbacks. First, linear regression assumes a linear relationship between variables, which often does not hold in reality. Additionally, linear models have limitations when handling multiple interaction terms. In our model, we selected a three-way interaction term by combining Candidate Names, Recency Weight, and State. This increases the complexity of the model, but linear regression struggles to automatically adapt to and capture these complex interactions, potentially leading to less accurate predictions.
Furthermore, linear regression is sensitive to outliers and noise in the data, making the model susceptible to instability due to these factors. It is also vulnerable to multicollinearity; for instance, Numeric Grade and Poll Score may have high correlations, which can make the model coefficients unreliable. This sensitivity and reliance on linear assumptions reduce the model’s ability to accurately predict complex, nonlinear relationships in the data.

## Future Study

In future research, if we continue to use a linear regression model to study U.S. election outcomes, we will focus on enhancing the model’s stability and generalization ability. We may consider using Lasso regression to reduce overfitting and address issues with model accuracy, ensuring that our model performs better when handling different or more complex datasets. Additionally, to prepare for the next election, we may introduce more data parameters and sources to improve the accuracy and applicability of election predictions, helping to decrease biases and false data caused by subjective factors. For example, we could incorporate demographic data, regional economic data, and social sentiment analysis to add parameters that reduce subjective influence, providing the model with a new perspective to analyze how different economic and cultural factors affect presidential choice across regions. 

Moreover, by analyzing voter characteristics, we could address the instability of swing state data by building a profile of swing state voters and their characteristics to better predict the outcome in these states. If linear regression proves insufficient, we may consider more complex models, such as Random Forests or Neural Networks, to better capture nonlinear relationships and complex interactions among variables. Ultimately, this approach could lead to a more accurate election prediction model.


\newpage

# Appendix a. {-}

## Overview of Emerson College Polling Methodology (October 23-25, 2024)

The Emerson College Polling conducted a survey from October 23 to 25, 2024, targeting 1,000 likely voters to investigate the differences in support for various candidates. In this presidential election, 58% support former President Donald Trump, while 39% support Vice President Kamala Harris.

## Population, Frame, and Sample

In this context, the target population consists of likely voters in the U.S. elections, defined by their likelihood to vote in the upcoming elections and their voting history, both of which are self-reported in the survey. The sampling frame specifically focuses on likely voters in Montana, who were reached through a combination of cell phone contacts provided by Aristotle and an online voter panel from CINT. The sample consists of 1,000 likely voters randomly selected from the sampling frame, with their status determined by a combination of voter history, registration status, and demographic data, all of which are self-reported. This methodology provides a balanced overview of Montana voters' priorities, with a credibility interval of +/- 3%.

## Sampling Approach and Trade-offs

Emerson College utilized a mixed-mode sampling approach for its poll of likely voters in Montana. This strategy involved two main methods: sending MMS text messages linked to an online survey using Aristotle’s voter lists and accessing a pre-screened, opt-in online panel from CINT. The MMS method is efficient and cost-effective, allowing participants to complete the survey at their convenience, which can enhance response rates. The online panel broadens coverage to include voters not reachable through text, capturing a wider demographic range across the state. Together, these methods create a diverse sample while reducing costs compared to traditional phone or in-person interviews.

However, this approach has trade-offs. The MMS survey requires recipients to have active cell phones and internet, potentially excluding older or less tech-savvy voters. Additionally, the online panel consists of self-selected participants, which may not fully reflect the general voter population. Mixing data from both sources can introduce inconsistencies, as each method may attract different respondent types, necessitating careful weighting to maintain balance and accuracy. Smaller demographic subsets, such as age, race, or education, carry higher credibility intervals due to reduced sample sizes, limiting precision in analysis. Overall, the mixed approach optimizes reach, reduces costs, and shows the priority needs of Montana’s voters, although there are limitations.

## Non-response Handling

Emerson College does not provide specific details regarding its non-response management. While it mentions that data were weighted by demographics such as gender, education, race, age, party registration, and region to align with the 2024 likely voter model, this weighting primarily addresses demographic imbalances and does not directly mitigate non-response bias. The survey lacks information on common non-response strategies, such as follow-up attempts, participation incentives, or specific adjustments for non-responders. This absence raises concerns about potential non-response bias, particularly if certain demographic groups were less likely to engage with the survey.

## Questionnaire Design

This questionnaire has strong points. Its straightforward and clear wording makes questions easy for respondents to follow and reduces potential confusion. By focusing on issues like the economy, housing, and voter approval for specific candidates, it captures key voter concerns in Montana, offering a concise view. The use of multiple questions around candidate approval, voter issues, and demographics adds depth to the questionnaire.
However, the questionnaire also has limitations. While demographic questions enhance the survey’s representativeness, smaller groups (e.g., nonbinary individuals) may carry higher credibility intervals, reducing precision for those subgroups. The mixed-mode approach (online panel and mobile) improves access but still risks non-response bias, as certain demographics might be less likely to participate. Overall, the design achieves clarity and breadth, though response biases and sample variations should be considered in interpreting the findings. For example, in this survey of 1,000 Montana voters, only 5 respondents identified as nonbinary or other genders. Since statistical reliability depends on the number of responses, small groups have higher variability, meaning their responses can swing widely due to each individual answer carrying greater weight.

# Appendix b. {-}

## Idealized Methodology for Forecasting the U.S. Presidential Election

We aim to develop a methodology for forecasting U.S. presidential election outcomes by conducting a survey with a $100,000 budget. Using stratified random sampling and multi-mode recruitment, the survey targets 10,000 likely voters across demographic and regional lines. Key measures include data validation checks, weighted analysis, and predictive modeling to ensure accuracy. Results will be enriched by aggregating reputable data sources like FiveThirtyEight for a better forecast.

## Budget Allocation

Funding allocations will focus on ensuring thorough and effective sampling, recruitment, data validation, and analysis methods are employed with a total budget of no more than $10,000. The proposed budget breakdown is as follows:

Survey platform costs: $10,000 (subscription fees for online survey tools such as Google Forms or Qualtrics)

Respondent incentives: $10,000 (gift cards or other incentives to encourage participation)

Recruitment and staffing: $35,000 (staffing costs for survey distribution and data collection)

Data analysis tools: $20,000 (statistical software licenses, data cleaning and analysis)

Marketing and promotion: $10,000 (awareness and engagement campaigns)

Contingency fund: $5,000 (for unexpected expenses)

## Sampling Approach

The sampling approach will employ a stratified random sampling method to ensure representation across various demographic groups, including age, gender, race, education level, geographical location, and party affiliation. The target population consists of likely voters in the U.S., defined by historical voting behavior and self-reported intentions to vote. A sample size of approximately 10,000 respondents will be aimed at ensuring statistical robustness and a credibility interval of +/- 1% at a 95% confidence level.
This will be achieved through a combination of national voter registration databases to identify potential respondents. For example, we can utilize the National Voter Registration Act (NVRA) data from the National Association of Secretaries of State (NASS) to access information on registered voters. This database will allow us to filter for likely voters based on their registration status and historical voting behavior, ensuring that our sampling frame is representative of the electorate. By using reliable sources, we can create a sampling framework that enhances the accuracy of our election forecasts.

## Respondent Recruitment and Data Validation

To reach the target population, a multi-mode recruitment strategy will be implemented:
Online Surveys: Use Google Forms to distribute the survey electronically.
Telephone Surveys: Conduct live telephone interviews to capture demographics that might not engage online.
Text Messaging Surveys: Implement SMS surveys to reach younger demographics and those without regular internet access.
Incentives: Offer gift cards or other small incentives for participation, particularly for online respondents. This can enhance response rates and engagement.

To effectively reach our target population and capture a diverse range of perspectives, we will implement a multi-mode recruitment strategy tailored to different demographic groups and communication preferences. This approach includes online surveys using a Google Forms questionnaire, which will be widely distributed through social media, email lists, and community networks to maximize reach among individuals who frequently engage online. The user-friendly platform allows participants to complete the survey quickly and anonymously on any internet-enabled device. The survey can be accessed through the following link: [https://forms.gle/oSbad52Vuw9Z9Wf46](https://forms.gle/oSbad52Vuw9Z9Wf46). Additionally, we will conduct live telephone interviews to include participants who may not be reachable through online channels, ensuring we capture responses from populations that might otherwise be underrepresented. To further engage younger demographics and individuals with limited internet access, we will implement SMS-based surveys, allowing participants to respond quickly via text. To encourage participation and improve response rates, small incentives such as digital gift cards will be offered, particularly for online respondents, with details communicated at the survey's start and awarded upon completion to ensure transparency. This comprehensive approach will enable us to gather a robust and representative dataset, providing valuable insights into the preferences and priorities of voters across multiple demographics.

## Poll Aggregation and Modeling

Poll results will be aggregated using statistical methods to identify trends and analyze historical voting patterns. For model building, we will implement a weighted analysis to ensure demographic representation, applying specific weights based on factors such as age, gender, race, and education level, as well as state significance to reflect regional variations in voter behavior. Predictive analytics will primarily involve logistic regression to model voting preferences and forecast election outcomes, supplemented by time-series analysis to track changes in voter sentiment over the campaign period. To enhance our findings, we will combine our data with reputable sources like FiveThirtyEight, utilizing their aggregation techniques to enrich our analysis. 

\newpage

# References

