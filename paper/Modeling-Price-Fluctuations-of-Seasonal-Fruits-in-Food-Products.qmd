---
title: "Modeling Price Change of Seasonal Fruits Flavored Food Products: A Predictive Analysis"
subtitle: "Trump's Narrow Victory Over Harris by Less Than One Percent of the Supporting Rate"
author: 
  - Yiyi Feng
thanks: "Code and data are available at: https://github.com/kqlqkqlqF/Insights-and-Predictions-for-the-U.S.-Election.git."
date: today
date-format: long
abstract: "This study presents a predictive model for the 2024 U.S. Presidential Election, focusing on the race between Donald Trump and Kamala Harris. Our model forecasts a narrow victory for Trump, estimating his average support at 44.51% compared to Harris's 43.86%, with leads of Trump in swing states. The analysis shows that state and recency are important for understanding voter support trends, reflecting the electoral system's winner-takes-all nature. This research allows electoral forecasting by demonstrating how localized support influences national outcomes and shows the need for improved polling methodologies."
format:
  pdf:
    toc: true
    number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

#### Preamble ####
# Purpose: Help constructing the "Predictive Modeling for Forecasting the 2024 US Presidential Election" paper

# Author: Bo Tang, Yiyi Feng, Mingjing Zhan
# Date: 1 November 2024
# Contact: qinghe.tang@mail.utoronto.ca, yiyi.feng@mail.utoronto.ca, mingjin.zhan@mail.utoronto.ca

####Workspace setup ####

library(dplyr)
library(tibble)
library(here)
library(modelsummary)
library(tidyverse)
library(arrow)
library(ggplot2)
library(janitor)
library(knitr)
library(kableExtra)
library(caret)
library(randomForest)
library(rstanarm)

model_data <- read_parquet(here::here("data/02-analysis_data/combined_model_data.parquet"))
rain_data <- read_parquet(here::here("data/02-analysis_data/average_rain_data.parquet"))

```


# Introduction

The upcoming U.S. Presidential Election marks an important point in the nation’s political landscape, shaped by public opinion, social and economic factors, and the complexities of the electoral process. With Kamala Harris and Donald Trump competing for the presidency, accurately predicting the outcome is increasingly important. Polls not only reflect voter opinion but also influence campaign strategies and media coverage. However, challenges like sampling biases, inconsistent methods, and the gap between the popular vote and the electoral vote emphasize the need for an improved forecasting model. This paper aims to develop a predictive framework that uses national polling data and examines state-level dynamics, especially in key swing states that often decide election results.

Our main focus is the probability of Donald Trump winning the 2024 U.S. presidential election, represented by voter support rates. To estimate support for both Trump and Harris, we developed linear models that account for factors such as candidate identity, poll recency, state, sample size, poll score, and poll quality, along with interactions among these variables. By identifying the optimal model, we aim to determine how these factors and their combinations influence expected support, providing insights into each candidate's chances across different regions and polling conditions. This approach models support rates rather than direct winning probabilities, allowing for a nuanced prediction that reflects variations by candidate, state, and recency.

Our model predicts that Donald Trump will win by a narrow margin, with an average support of 44.51% compared to Kamala Harris's 43.86%. Trump leads in six out of seven key swing states, suggesting that this localized support could enhance his overall chances despite only a slight national lead. Among the predictor variables analyzed, state and recency are the most significant indicators of support trends, reflecting the "winner-takes-all" nature of the U.S. electoral system and the increasing accuracy of polling data as Election Day approaches.

The remainder of this paper is structured as follows: [@sec-data] provides an overview of the dataset, details of the parameters, outcome and predictor variables, and the packages used during processing. [@sec-model] explains the modeling approach, and best model selection, justifying the choice of predictors and outlining the methods used to forecast support for Trump and Harris. [@sec-results] presents the findings, including a summary of the predicted support rate for Trump, a comparison of the predicted support rates for Trump and Harris, and a breakdown of their support rates in each state. In [@sec-discussion], we discuss the implications of these results, the limitations of our analysis, and potential avenues for future research. Additional methodological details and diagnostics are included in the appendix.



# Data {#sec-data}

## Overview

In this analysis, we used R [@citeR] to investigate canadian grocery price data. Our dataset, sourced from Jacob Filipp's project hammer [@hammer], provided the change of canadian grocery price from 8 vendors: Voila, T&T, Loblaws, No Frills, Metro, Galleria, Walmart and Save-On-Foods, from February 28, 2024 to November 26, 2024 (when this article was written). We examined factors that might influencing the price of seasonal fruits flavored food, including vendor, rain fall, month and category of the food.

Several R packages were vital for our data manipulation, modeling, and visualization efforts. The dplyr package provided efficient tools for data transformation and summarization [@dplyr], while modelsummary enhanced the presentation of model outputs in a clear and organized manner [@modelsummary]. kableExtra created customizable tables to improve our data presentation [@kableExtra]. Finally, testthat ensured the reliability of our analyses through code testing [@testthat]. Our workflow closely adhered to best practices, as outlined in [@tellingstories], enhancing the robustness of our predictive framework.
--------------------------------------

## Measurement
	

In this section, we describe the process of transforming the raw Canadian grocery price data into a structured dataset for analysis. Since this study focuses on price changes in foods with seasonal fruit flavors, we selected bananas and strawberries as representatives. Bananas were chosen for their popularity in winter, and strawberries for spring. Compared to other seasonal fruits like watermelon and pomegranate, bananas and strawberries have a larger consumer base in Canada, making it easier to collect relevant data for model building.

The original Canadian grocery price data was collected by Jacob Filipp through screen-scraping the website interfaces of eight vendors and compiled into the \textbf{Project Hammer} dataset. This dataset contains two tables: \textbf{Hammer 4 Raw} and \textbf{Hammer 4 Product}.
\textbf{Hammer 4 Raw} includes the scrape time, product name, single-item price, unit price (e.g., price per 100g), past prices, additional information (e.g., availability or discounts), and a unique product ID.
\textbf{Hammer 4 Product} provides detailed information, such as brand, vendor, sales unit, and product detail page links.
Since the data comes from website scraping, it contains many gaps and ambiguities, making cleaning difficult. Details about data collection and cleaning are provided in Appendix A.

We have considered sources like Statistics Canada, the Retail Council of Canada, and Open Data Portals as potential data providers. However, after reviewing their datasets, we found they do not meet the needs of our study. These datasets focus on broad food categories rather than specific flavored products like strawberry or banana. They are also ganeralized on time scale, lacking the specific needed to track monthly price changes or vendor-level details, which are critical for modeling seasonal price changes. Therefore, we determined these sources are unsuitable for our research.

To better analyze price changes for banana and strawberry-flavored foods, we added data from the \textbf{"About Rain Gauge Locations and Precipitation"} dataset from Open Data Toronto []. This dataset includes rain gauge locations across Toronto and recorded precipitation. Rainfall was included as a predictor since consumer demand for seasonal fruits often correlates with weather conditions, such as a preference for watermelon and strawberries during dry summers []. The dataset only uses rainfall data because the original grocery price data lacks sales location details.

This structured dataset enables us to analyze trends in seasonal fruit-flavored food prices over time. The goal is to examine price trends for these foods during in-season and off-season sales and identify key influencing factors.


## Outcome Variables

### Change of Monthly Averaged Price for Banana and Strawberry Flavored Product

[@fig-one] illustrates the monthly average price distribution of banana and strawberry-flavored products. Most products are priced between 0 and 10 dollars, with very few exceeding 10 dollars. This indicates that the majority of sales fall within the lower price range, with prices above 20 dollars being rare. The average price distribution of strawberry and banana-flavored products shows no significant difference, though strawberry products have a slightly smaller proportion in the 0 to 10 dollar range compared to banana products.

```{r}
#| label: fig-one
#| fig-cap: Distribution of Monthly Averaged Price for Banana and Strawberry Flavored Product
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Create the plot for average price distribution
ggplot(model_data, aes(x = monthly_avg_price)) +
  geom_histogram(aes(y = ..density..),
                 bins = 30,
                 fill = "blue",
                 color = "black",
                 alpha = 0.7
  ) +
  geom_density(color = "black", fill = "salmon", alpha = 0.6) +
  labs(
    x = "Average Price",
    y = "Density"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    strip.text = element_text(size = 12), # Adjust facet label size
    axis.title = element_text(size = 14), # Adjust axis title size
    axis.text = element_text(size = 12)  # Adjust axis label size
  )


```

```{r}
#| label: fig-two
#| fig-cap: Distribution of Monthly Averaged Price Change for Banana and Strawberry Flavored Product from July 2024 to Novermber 2024
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Create the plot for price distribution by month
ggplot(model_data, aes(x = as.factor(month), y = price_change, color = price_change)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Month",
    y = "Average Price"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12)
  )


```

[@fig-two] illustrates the price change of monthly average distribution of banana and strawberry-flavored products. Overall, strawberry-flavored products show greater price fluctuation compared to banana-flavored ones. Banana product prices typically vary within a range of ±1.5 dollars, while strawberry prices fluctuate within ±2.5 dollars. However, neither flavor shows a clear trend of overall price increases or decreases over the months.


This contradicts our hypothesis: we anticipated that banana-flavored product prices might increase between July and November, as bananas are a popular fall and winter flavor, while strawberry-flavored product prices might decline during this period, given their popularity as a summer flavor.


## Predictor Variables

### Summary of Predictor Variables

- **Vendor:** Canada's eight major suppliers include Voila, T&T, Loblaws, No Frills, Metro, Galleria, Walmart, and Save-On-Foods.

- **Average Rainfall Per Month:** Using the precipitation data from the "About Rain Gauge Locations and Precipitation" dataset, we calculated the monthly average rainfall for Toronto from July to November 2024, with measurements in millimeters.

- **Food Category:** We categorized banana and strawberry-flavored foods into five groups: beverage, yogurt, flavored tea, solid snack, and fruit. The "beverage" category includes all drinks except tea and yogurt-based beverages. The "flavored tea" category encompasses both tea drinks and flavored tea bags, while "fruit" refers to various types of strawberry or banana fruit.

- **Month:** Month records the corresponding month information when the price data for each product was collected. In this dataset, it starts from July to November.

### Vendor

Based on [@fig-vendor], we observed that the overall number of strawberry-flavored products is significantly higher than that of banana-flavored products. This is because the original dataset shows that the quantity of strawberry-flavored products is roughly 3-4 times greater than that of banana-flavored products, indicating that strawberry-flavored items have a broader audience. Save-On-Foods has the lowest share in both flavor categories, suggesting that they either offer fewer food types overall or focus on selling other product categories rather than fruit-flavored foods.

```{r}
#| label: fig-vendor
#| fig-cap: Distribution of the Counts of Banana and Strawberry Flavored Food Offered in Each Vendor
#| echo: false
#| warning: false
#| message: false

# Create a bar plot for vendor distribution
ggplot(model_data, aes(x = vendor)) +
  geom_bar(fill = "blue", color = "black", alpha = 0.7) + 
  labs(
    x = "Vendor",
    y = "Count"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels for better readability
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )


```

### Average Rainfall Per Month

[@fig-rain] shows the average monthly rainfall in Toronto, measured in millimeters. It is evident that October had the lowest rainfall, while July had the highest. From July to October, there is a continuous downward trend, with a rebound in November. We chose rainfall as a predictor because it is highly correlated with time and seasonal changes, and we believe it will be helpful in our model.

```{r}
#| label: fig-rain
#| fig-cap: Average Rainfall in Toronto from July 2024 to November 2024
#| echo: false
#| warning: false
#| message: false

# Assuming rain_data is your dataset
ggplot(rain_data, aes(x = as.factor(month), y = avg_rainfall)) +
  geom_bar(stat = "identity", fill = "blue", color = "black", alpha = 0.7) + 
  labs(
    x = "Month",
    y = "Average Rainfall (mm)",
    title = "Average Rainfall per Month"
  ) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )


```

### Food Category

In [@fig-category], we can see that solid snacks have the highest overall prices, reaching up to 80 dollars per unit, while flavored tea has the lowest overall prices. Additionally, the price distribution for solid snacks, fruit, and beverages is relatively dispersed, whereas yogurt and flavored tea show more tightly clustered price distributions. We believe this is because flavored tea and yogurt are more specialized categories compared to the others.

```{r}
#| label: fig-category
#| fig-cap: Distribution of Monthly Averaged Price for Banana and Strawberry Flavored Product within Category
#| echo: false
#| warning: false
#| message: false
# Convert pct column to numeric if it is not already

# Create the plot for price distribution by category
ggplot(model_data, aes(x = category, y = monthly_avg_price, color = monthly_avg_price)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Category",
    y = "Average Price"
  ) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12)
  )


```

### Month

[@fig-month] shows the distribution of the Monthly Averaged Price for Banana and Strawberry Flavored Products over time. It is clear that strawberry-flavored products are priced higher than banana-flavored products in all months. However, neither flavor shows a noticeable increasing or decreasing trend in price distribution across the months.

```{r}
#| label: fig-month
#| fig-cap: Distribution of Monthly Averaged Price for Banana and Strawberry Flavored Product from July 2024 to Novermber 2024
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Create the plot for price distribution by month
ggplot(model_data, aes(x = as.factor(month), y = monthly_avg_price, color = monthly_avg_price)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Month",
    y = "Average Price",
    title = "Average Price Distribution by Month"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12)
  )

```


# Model {#sec-model}

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

model_rf <-
  readRDS(file = here::here("models/model_rf.rds"))

model_linear_1 <-
  readRDS(file = here::here("models/model_linear_1.rds"))

model_linear_2 <-
  readRDS(file = here::here("models/model_linear_2.rds"))

model_linear_3 <-
  readRDS(file = here::here("models/model_linear_3.rds"))

model_linear_4 <-
  readRDS(file = here::here("models/model_linear_4.rds"))

model_linear_5 <-
  readRDS(file = here::here("models/model_linear_5.rds"))


```


The goal of this section is to build a robust predictive model for price changes in banana and strawberry flavored products. The key challenge lies in ensuring the model effectively captures the dynamics of price changes despite limited sample size and temporal coverage. To address this, we evaluated multiple model specifications to identify the one that best aligns with our predictive objectives.

We chose to model the monthly average unit price changes for banana and strawberry flavored products rather than making more precise predictions, such as price per 100g. This decision was driven by the original dataset's limitations, which include a variety of measurement units (e.g., per 100g, per 100ml, per 1L, per 1kg, per lb) and the fact that many products only provide prices for each product without corresponding measurement details. As a result, we focused on unit prices, acknowledging that this might reduce the model's performance.

We incorporated Vendor, Food Category, Average Rainfall, and Month as predictors. Recognizing potential interactions among these variables. For example, rainfall and month are typically strongly correlated, and food category might interact with both month and vendor. We developed a series of linear models for the prediction. By systematically comparing models with different interaction terms, we aimed to identify the best-performing model to deliver reliable predictions.

## Model Set-up

Our goal is to model the price changes of strawberry and banana flavored products based on Vendor, Food Category, Average Rainfall, and Month. The model incorporates interaction terms to capture how combinations of these factors jointly influence price changes, providing a clearer understanding of which factors most significantly affect the pricing of seasonal fruit flavored products.

\
\begin{align*}
\mathrm{Pct}_i = &\ \beta_0 + \beta_1 \cdot \mathrm{Vendor}_i + \beta_2 \cdot \mathrm{Category}_i + \beta_3 \cdot \mathrm{Month}_i + \beta_4 \cdot \mathrm{Average Rainfall}_i \\
& + \beta_5 \cdot (\mathrm{Vendor}_i \times \mathrm{Category}_i) + \beta_6 \cdot (\mathrm{Category}_i \times \mathrm{Month}_i) \\
& + \beta_7 \cdot (\mathrm{Month}_i \times \mathrm{Average Rainfall}_i) + \beta_8 \cdot (\mathrm{Category}_i \times \mathrm{Average Rainfall}_i) + \epsilon_i
\end{align*}
\

Where

-   $y_i$ : The percentage of support for candidate in poll i.
-   $\beta_0$: Intercept term, representing the predicted `Average price of the Product` when all independent variables are 0.
-   $\beta_1$: Main effect of `Vendor`, capturing the influence of the candidate.
-   $\beta_2$: Main effect of `Category`, reflecting the influence of how recent the poll is on `Average price of the Product`.
-   $\beta_3$: Main effect of `Month`, indicating the impact of different states on `Average price of the Product`.
-   $\beta_4$: Main effect of `Average Rainfall`, indicating the impact of different states on `Average price of the Product`.
-   $\beta_5$: Interaction effect between `Vendor` and `Category`, capturing the combined influence of the candidate and state.
-   $\beta_6$: Interaction effect between `Category` and `Month`, reflecting the joint impact of recency and state on `pct`.
-   $\beta_7$: Interaction effect between `Month` and `Average rainfall`, representing the combined influence of the candidate and recency of the poll.
-   $\beta_8$: Interaction effect between `Category` and `Average rainfall`, representing the combined influence of the candidate and recency of the poll.
-   $\epsilon_i$: The error term, assumed to follow a normal distribution with mean 0.

### Model Interpretation

This regression model aims to predict price changes for strawberry and banana flavored products by incorporating both main effects and interaction terms. The intercept represents the baseline price change when all predictors are zero. Main effects include vendor, average rainfall, food category, and month, capturing how each factor independently influences price. For example, as the months progress into winter, strawberry flavored products, often associated with summer, may see a price drop.

To better understand more complex dynamics, the model includes two-way interaction terms, such as vendor and category, category and month, and month and average rainfall. These interactions reveal how factors influence one another. For instance, vendors may focus on specific product types, such as selling more beverages than fruits. The interaction between category and month reflects seasonal preferences, like reduced demand for beverages during Canada’s cold winters. The interaction between rainfall and month highlights their direct relationship, with June being Canada’s wettest month and November its driest.

Finally, the model includes an error term to account for unexplained variability, enhancing its predictive reliability. By combining both individual and interdependent relationships, the model offers a nuanced approach to forecasting price changes for these products.

## Model Justification

```{r}
#| label: tbl-linear
#| tbl-cap: "Predicted Average Supporting Percentages for Donald Trump vs. Kamala Harris by Random Forest"
#| echo: false
#| warning: false
#| message: false

# Extract summary statistics and add variables included in each model
model_summary <- tibble(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5"),
  Variables = c(
    linebreak("Vendor, Category, Month, Average Rainfall"), 
    linebreak("Vendor, Category × Month, Average Rainfall"), 
    linebreak("Vendor, Category, Month × Average Rainfall"),
    linebreak("Vendor, Month, Category × Average Rainfall"),
    linebreak("Vendor × Category, Month, Average Rainfall")
  ),
  `R²` = round(c(summary(model_linear_1)$r.squared, 
                 summary(model_linear_2)$r.squared, 
                 summary(model_linear_3)$r.squared,
                 summary(model_linear_4)$r.squared,
                 summary(model_linear_5)$r.squared), 5),
  `Adjusted R²` = round(c(summary(model_linear_1)$adj.r.squared,
                          summary(model_linear_2)$adj.r.squared, 
                          summary(model_linear_3)$adj.r.squared,
                          summary(model_linear_4)$adj.r.squared,
                          summary(model_linear_5)$adj.r.squared), 5),
  `AIC` = round(c(AIC(model_linear_1), AIC(model_linear_2), AIC(model_linear_3), AIC(model_linear_4), AIC(model_linear_5)), 5),
  `BIC` = round(c(BIC(model_linear_1), BIC(model_linear_2), BIC(model_linear_3), BIC(model_linear_4), BIC(model_linear_5)), 5),
  `RMSE` = round(c(sqrt(mean(residuals(model_linear_1)^2)), 
                   sqrt(mean(residuals(model_linear_2)^2)), 
                   sqrt(mean(residuals(model_linear_3)^2)),
                   sqrt(mean(residuals(model_linear_4)^2)),
                   sqrt(mean(residuals(model_linear_5)^2))), 5)
)

# Display the table using kable
model_summary %>%
  kable(digits = 5, escape = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  column_spec(2, width = "8em")


```



[@tbl-linear] summarizes the performance metrics for five models, each with different interactions, while the first model contains no interaction.

Model 1, which includes basic predictors (Vendor, Category, Month, and Average Rainfall), exhibits poor predictive performance with an R² of 0.00383 and an RMSE of 0.50983, indicating limited explanatory power. Model 2 improves slightly by incorporating an interaction between Category and Month, increasing R² to 0.00890 and lowering RMSE to 0.50853, suggesting minor gains in prediction accuracy. Model 3, which includes an interaction between Month and Average Rainfall, yields no improvement over Model 1, with identical R² (0.00383) and RMSE (0.50983), highlighting the limited value of this interaction.

Model 4 adds an interaction between Category and Average Rainfall, resulting in a marginally higher R² of 0.00431 but with negligible impact on RMSE (0.50971). Finally, Model 5 introduces an interaction between Vendor and Category, leading to an R² of 0.00821 and an RMSE of 0.50871, showing slight improvement but still limited overall explanatory power.

In conclusion, none of the models provide strong predictive accuracy, but Model 2 achieves the best balance among them with the highest R² (0.00890) and lowest RMSE (0.50853). However, the overall low R² values indicate that these predictors and interactions capture only a small portion of the variation in price changes.

## Optimization

```{r}
#| label: tbl-rfsum
#| tbl-cap: Model Summary with Included Variables and Interactions
#| echo: false
#| warning: false
#| message: false

# Extract OOB error
oob_error <- model_rf$mse[model_rf$ntree]

# Number of trees
num_trees <- model_rf$ntree

# Extract variable importance
var_importance <- importance(model_rf)
importance_table <- as_tibble(var_importance, rownames = "Variable") %>%
  rename(
    `%IncMSE` = `%IncMSE`,
    `IncNodePurity` = `IncNodePurity`
  )

# Construct a summary table for model metrics
model_metrics <- tibble(
  Metric = c("Number of Trees", "Out-of-Bag Error (MSE)"),
  Value = c(num_trees, round(oob_error, 5))
)

# Display model metrics table
model_metrics %>%
  kable(digits = 5, escape = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "Random Forest Model Metrics" = 1)) %>%
  column_spec(1, bold = TRUE, width = "12em")

# Display variable importance table
importance_table %>%
  mutate(
    `%IncMSE` = round(`%IncMSE`, 2),
    `IncNodePurity` = round(`IncNodePurity`, 2)
  ) %>%
  kable(digits = 2, escape = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "Variable Importance in Random Forest" = 2)) %>%
  column_spec(1, bold = TRUE, width = "12em")


```

Due to the limitations of linear models in handling interaction terms, we propose an optimized model—Random Forest. The consistently poor performance of linear models suggests that a linear relationship may not adequately explain the price changes of banana and strawberry flavored products in relation to the predictors. We chose Random Forest because our data includes interaction terms, and linear analysis is not suitable for such data. Random Forest excels at handling interactions and nonlinear relationships, as it can automatically identify and leverage these complex interactions through its decision tree structure without the need for manual adjustments. In contrast, linear models have limited capacity to handle interactions, requiring manual specification, and are less effective at capturing nonlinear effects.

[@tbl-rfsum] presents the prediction results obtained using Random Forest.

Compared to Linear Model 2, the best performing linear model, the Random Forest model demonstrates significantly better predictive accuracy. Model 2, which incorporates basic predictors (Vendor, Category, Month, and Average Rainfall) along with an interaction between Category and Month, achieves an R² of 0.00890 and an RMSE of 0.50853, suggesting minimal improvement in prediction accuracy. In contrast, the Random Forest model, with 500 trees and an Out-of-Bag MSE of 0.25972, delivers a substantial reduction in error and appears to capture more of the underlying complexity in the data.

While the linear models struggle to account for interactions and non-linear relationships, the Random Forest model effectively handles these through its decision tree structure. This allows it to automatically detect and utilize complex interactions between variables, leading to a much better fit. In conclusion, the Random Forest model outperforms Model 2 by a significant margin, providing stronger predictive power and better handling of the data's complexity.

[@tbl-rfsum] also presents the variable importance metrics for the Random Forest model, including both the %IncMSE and IncNodePurity values, which help identify how each predictor contributes to the model’s performance.

The Category variable stands out as the most important predictor, with a %IncMSE of 19.34 and an IncNodePurity of 5.63. This suggests that changes in food category have the greatest impact on the model’s predictive accuracy and contribute strongly to reducing the impurity of the decision nodes in the trees. Vendor follows closely, with a %IncMSE of 13.52 and an IncNodePurity of 7.73, indicating that the vendor also plays a significant role in determining price changes, though slightly less than category.

Month and Average Rainfall also show notable importance, with %IncMSE values of 14.32 and 11.44, respectively. These variables contribute moderately to the model's predictive accuracy, with month playing a key role in capturing seasonal effects and rainfall influencing potential price fluctuations related to environmental factors.

Finally, Flavor exhibits the lowest importance, with a negative %IncMSE of -1.11, which suggests that including flavor as a predictor may actually decrease the model’s accuracy, although its IncNodePurity value of 1.34 shows it still holds some relevance in the decision-making process.

In conclusion, Category and Vendor are the most influential variables in the Random Forest model, while Flavor has the least impact, possibly detracting from the overall performance. These insights help refine the model and highlight the factors that most strongly influence price variations.

# Result {#sec-results}

```{r}
#| label: tbl-resultone
#| tbl-cap: "Summary Statistics of Predicted Support for Donald Trump"
#| message: false
#| echo: false
#| warning: false

# Step 1: Predict price changes for the existing dataset
predict_data_lm <- model_data %>%
  mutate(Predicted_Price_Change = predict(model_linear_2, newdata = model_data))

# Step 2: Filter the dataset for 'strawberry' and 'banana' flavor products
flavor_data <- predict_data_lm %>%
  filter(flavor %in% c("strawberry", "banana"))

# Step 3: Calculate average predicted price change for each flavor across the dataset
flavor_summary <- flavor_data %>%
  group_by(flavor) %>%
  summarise(
    Avg_Price_Change = mean(Predicted_Price_Change, na.rm = TRUE),
    Median_Price_Change = median(Predicted_Price_Change, na.rm = TRUE),
    Min_Price_Change = min(Predicted_Price_Change, na.rm = TRUE),
    Max_Price_Change = max(Predicted_Price_Change, na.rm = TRUE),
    SD_Price_Change = sd(Predicted_Price_Change, na.rm = TRUE),
    Total_Products = n()
  )

# Step 4: Present the summary using kable
flavor_summary %>%
  kable(
    col.names = c("Flavor", "Avg Change", "Median Change", "Min Change", "Max Change", "SD of Change", "Sample Size"),
    digits = 3,
    booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "scale_down"))



```

[@tbl-resultone] summarizes the price change statistics for banana and strawberry flavored products.

Banana flavored products have an average price change of 0.009, with a median change of 0.018 across 1,376 products. Strawberry-flavored products show a slightly higher average change of 0.010, with a median of 0.021. For both of the flavors, their price changes  range from -0.109 to 0.112. However, for banana flavored product, their sample size is roughly only a quarter of strawberry product sample size.Overall, strawberry-flavored products exhibit slightly higher average price changes and more variation compared to banana-flavored products.

```{r}
#| label: fig-lmone
#| fig-cap: This figure shows the distribution of monthly price changes for banana and strawberry products. A higher concentration around zero indicates more stable prices, while wider distributions suggest greater variability.
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Create the plot for price distribution by month
ggplot(predict_data_lm, aes(x = as.factor(month), y = Predicted_Price_Change, color = Predicted_Price_Change)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Month",
    y = "Average Price",
    title = "Average Price Distribution by Month"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12)
  ) 

```

[@tbl-modelresults] presents the model's average predicted supporting percentages for Donald Trump and Kamala Harris, along with their normalized percentages. According to the model's predictions, Donald Trump's average predicted supporting percentage is 44.51%, while Kamala Harris's is 43.86%. The normalized percentages adjust these predicted values to relative proportions, with Donald Trump at 50.37% and Kamala Harris at 49.63%. These results show that although Donald Trump's average predicted supporting percentage is slightly higher than Kamala Harris's, the support rates for both candidates are nearly equal, with Donald Trump holding a slight edge.

```{r}
#| label: fig-lmtwo
#| fig-cap: This figure shows the distribution of monthly price changes for banana and strawberry products. A higher concentration around zero indicates more stable prices, while wider distributions suggest greater variability.
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Create the plot for price distribution by month
ggplot(predict_data_lm, aes(x = category, y = Predicted_Price_Change, color = Predicted_Price_Change)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Month",
    y = "Average Price",
    title = "Average Price Distribution by Month"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis labels
  )

```



```{r}
#| label: fig-rfone
#| fig-cap: This figure shows the distribution of monthly price changes for banana and strawberry products. A higher concentration around zero indicates more stable prices, while wider distributions suggest greater variability.
#| echo: false
#| eval: true
#| warning: false
#| message: false

predict_data_rf <- model_data %>%
  mutate(predicted_price_change = predict(model_rf, newdata = model_data))

# Create the plot for price distribution by month
ggplot(predict_data_rf, aes(x = as.factor(month), y = predicted_price_change, color = predicted_price_change)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Month",
    y = "Average Price",
    title = "Average Price Distribution by Month"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12)
  )


```

[@fig-map] shows the predicted winner by state. Red indicates Trump’s predicted lead, blue indicates Harris’s lead, and gray marks states with insufficient data to predict the winner. This map visually represents regional support patterns for both candidates across the country. Trump shows strength in parts of the Midwest and South, while Harris performs better in parts of the Northeast and West. The table below [@tbl-model-state] presents the specific support rate figures.

```{r}
#| label: fig-rftwo
#| fig-cap: This figure shows the distribution of monthly price changes for banana and strawberry products. A higher concentration around zero indicates more stable prices, while wider distributions suggest greater variability.
#| echo: false
#| eval: true
#| warning: false
#| message: false

predict_data_rf <- model_data %>%
  mutate(predicted_price_change = predict(model_rf, newdata = model_data))

# Create the plot for price distribution by month
ggplot(predict_data_rf, aes(x = category, y = predicted_price_change, color = predicted_price_change)) +
  geom_violin(fill = "blue", alpha = 0.4) +  # Use violin plot to show the distribution of the average prices
  geom_jitter(width = 0.1, alpha = 0.7) +  # Scatter plot for individual data points, jittered for visibility
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient based on the price
  labs(
    x = "Month",
    y = "Average Price",
    title = "Average Price Distribution by Month"
  ) +
  theme_minimal() +
  facet_wrap(~flavor) +
  theme(
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    strip.text = element_text(size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis labels
  )


```

[@tbl-model-state] provides detailed data for each state’s predicted support rate and expected winner. Together with [@fig-map], it offers a better understanding of Trump and Harris’s support levels in each state.

# Discussion {#sec-discussion}

## Summery of Findings

Our model predicts a narrow victory for Donald Trump, with an average support of 44.51% compared to Kamala Harris’s 43.86%. This result aligns with the predictions from our Random Forest model in [@tbl-rf], which also suggests a slight edge for Trump. However, since a higher popular vote does not guarantee an election win, and both models indicate no significant lead, we hesitate to conclude that Trump is highly likely to win.

Looking at support across states, Trump has a clear advantage in the swing states, which strengthens his position in our prediction. Out of seven swing states: Wisconsin, Pennsylvania, North Carolina, Nevada, Michigan, Georgia, and Arizona [@swing], Trump leads six. Since swing states play an important role in the election outcome, we believe that even with a slight national lead, Trump’s support in these states could give him an advantage over Harris.

Among all predictor variables, state and the recency of poll data stand out as the most effective indicators of support trends. This finding aligns with structural aspects of the U.S. election system: most states use a “winner-takes-all” approach, where winning the majority yields all electoral votes, and many states have strong historical party preferences. Consequently, state-level support has a direct impact on the election outcome. Additionally, as Election Day nears, polling data becomes increasingly accurate, better reflecting actual voter intentions.

## Limitation

Our analysis focuses exclusively on data related to Trump, which may weaken the overall analysis. In organizing the data, we excluded poll data with low numeric grades to enhance its credibility; however, this approach resulted in a smaller sample size and reduced coverage. When examining the relationship between state and supporting percentage (PCT), we did not integrate PCT with sample size for comparison. We believe that all decisions made by the collective data should be treated equally, regardless of sample size. This perspective has led to some counterintuitive results, such as states where Trump’s party is strong giving him fewer votes. Additionally, a significant portion of our raw data lacks exact state labels, which introduces errors in analyzing the relationship between state and PCT, even after processing.

Choosing a linear regression model to predict election results has several clear drawbacks. First, linear regression assumes a linear relationship between variables, which often does not hold in reality. Additionally, linear models have limitations when handling multiple interaction terms. In our model, we selected a three-way interaction term by combining Vendors, Category, and State. This increases the complexity of the model, but linear regression struggles to automatically adapt to and capture these complex interactions, potentially leading to less accurate predictions.
Furthermore, linear regression is sensitive to outliers and noise in the data, making the model susceptible to instability due to these factors. It is also vulnerable to multicollinearity; for instance, Numeric Grade and Poll Score may have high correlations, which can make the model coefficients unreliable. This sensitivity and reliance on linear assumptions reduce the model’s ability to accurately predict complex, nonlinear relationships in the data.

## Future Study

In future research, if we continue to use a linear regression model to study U.S. election outcomes, we will focus on enhancing the model’s stability and generalization ability. We may consider using Lasso regression to reduce overfitting and address issues with model accuracy, ensuring that our model performs better when handling different or more complex datasets. Additionally, to prepare for the next election, we may introduce more data parameters and sources to improve the accuracy and applicability of election predictions, helping to decrease biases and false data caused by subjective factors. For example, we could incorporate demographic data, regional economic data, and social sentiment analysis to add parameters that reduce subjective influence, providing the model with a new perspective to analyze how different economic and cultural factors affect presidential choice across regions. 

Moreover, by analyzing voter characteristics, we could address the instability of swing state data by building a profile of swing state voters and their characteristics to better predict the outcome in these states. If linear regression proves insufficient, we may consider more complex models, such as Random Forests or Neural Networks, to better capture nonlinear relationships and complex interactions among variables. Ultimately, this approach could lead to a more accurate election prediction model.


\newpage

# Appendix a. {-}

## Overview of Emerson College Polling Methodology (October 23-25, 2024)

The Emerson College Polling conducted a survey from October 23 to 25, 2024, targeting 1,000 likely voters to investigate the differences in support for various candidates. In this presidential election, 58% support former President Donald Trump, while 39% support Vice President Kamala Harris.

## Population, Frame, and Sample

In this context, the target population consists of likely voters in the U.S. elections, defined by their likelihood to vote in the upcoming elections and their voting history, both of which are self-reported in the survey. The sampling frame specifically focuses on likely voters in Montana, who were reached through a combination of cell phone contacts provided by Aristotle and an online voter panel from CINT. The sample consists of 1,000 likely voters randomly selected from the sampling frame, with their status determined by a combination of voter history, registration status, and demographic data, all of which are self-reported. This methodology provides a balanced overview of Montana voters' priorities, with a credibility interval of +/- 3%.

## Sampling Approach and Trade-offs

Emerson College utilized a mixed-mode sampling approach for its poll of likely voters in Montana. This strategy involved two main methods: sending MMS text messages linked to an online survey using Aristotle’s voter lists and accessing a pre-screened, opt-in online panel from CINT. The MMS method is efficient and cost-effective, allowing participants to complete the survey at their convenience, which can enhance response rates. The online panel broadens coverage to include voters not reachable through text, capturing a wider demographic range across the state. Together, these methods create a diverse sample while reducing costs compared to traditional phone or in-person interviews.

However, this approach has trade-offs. The MMS survey requires recipients to have active cell phones and internet, potentially excluding older or less tech-savvy voters. Additionally, the online panel consists of self-selected participants, which may not fully reflect the general voter population. Mixing data from both sources can introduce inconsistencies, as each method may attract different respondent types, necessitating careful weighting to maintain balance and accuracy. Smaller demographic subsets, such as age, race, or education, carry higher credibility intervals due to reduced sample sizes, limiting precision in analysis. Overall, the mixed approach optimizes reach, reduces costs, and shows the priority needs of Montana’s voters, although there are limitations.

## Non-response Handling

Emerson College does not provide specific details regarding its non-response management. While it mentions that data were weighted by demographics such as gender, education, race, age, party registration, and region to align with the 2024 likely voter model, this weighting primarily addresses demographic imbalances and does not directly mitigate non-response bias. The survey lacks information on common non-response strategies, such as follow-up attempts, participation incentives, or specific adjustments for non-responders. This absence raises concerns about potential non-response bias, particularly if certain demographic groups were less likely to engage with the survey.

## Questionnaire Design

This questionnaire has strong points. Its straightforward and clear wording makes questions easy for respondents to follow and reduces potential confusion. By focusing on issues like the economy, housing, and voter approval for specific candidates, it captures key voter concerns in Montana, offering a concise view. The use of multiple questions around candidate approval, voter issues, and demographics adds depth to the questionnaire.
However, the questionnaire also has limitations. While demographic questions enhance the survey’s representativeness, smaller groups (e.g., nonbinary individuals) may carry higher credibility intervals, reducing precision for those subgroups. The mixed-mode approach (online panel and mobile) improves access but still risks non-response bias, as certain demographics might be less likely to participate. Overall, the design achieves clarity and breadth, though response biases and sample variations should be considered in interpreting the findings. For example, in this survey of 1,000 Montana voters, only 5 respondents identified as nonbinary or other genders. Since statistical reliability depends on the number of responses, small groups have higher variability, meaning their responses can swing widely due to each individual answer carrying greater weight.

# Appendix b. {-}

## Idealized Methodology for Forecasting the U.S. Presidential Election

We aim to develop a methodology for forecasting U.S. presidential election outcomes by conducting a survey with a $100,000 budget. Using stratified random sampling and multi-mode recruitment, the survey targets 10,000 likely voters across demographic and regional lines. Key measures include data validation checks, weighted analysis, and predictive modeling to ensure accuracy. Results will be enriched by aggregating reputable data sources like FiveThirtyEight for a better forecast.

## Budget Allocation

Funding allocations will focus on ensuring thorough and effective sampling, recruitment, data validation, and analysis methods are employed with a total budget of no more than $10,000. The proposed budget breakdown is as follows:

Survey platform costs: $10,000 (subscription fees for online survey tools such as Google Forms or Qualtrics)

Respondent incentives: $10,000 (gift cards or other incentives to encourage participation)

Recruitment and staffing: $35,000 (staffing costs for survey distribution and data collection)

Data analysis tools: $20,000 (statistical software licenses, data cleaning and analysis)

Marketing and promotion: $10,000 (awareness and engagement campaigns)

Contingency fund: $5,000 (for unexpected expenses)

## Sampling Approach

The sampling approach will employ a stratified random sampling method to ensure representation across various demographic groups, including age, gender, race, education level, geographical location, and party affiliation. The target population consists of likely voters in the U.S., defined by historical voting behavior and self-reported intentions to vote. A sample size of approximately 10,000 respondents will be aimed at ensuring statistical robustness and a credibility interval of +/- 1% at a 95% confidence level.
This will be achieved through a combination of national voter registration databases to identify potential respondents. For example, we can utilize the National Voter Registration Act (NVRA) data from the National Association of Secretaries of State (NASS) to access information on registered voters. This database will allow us to filter for likely voters based on their registration status and historical voting behavior, ensuring that our sampling frame is representative of the electorate. By using reliable sources, we can create a sampling framework that enhances the accuracy of our election forecasts.

## Respondent Recruitment and Data Validation

To reach the target population, a multi-mode recruitment strategy will be implemented:
Online Surveys: Use Google Forms to distribute the survey electronically.
Telephone Surveys: Conduct live telephone interviews to capture demographics that might not engage online.
Text Messaging Surveys: Implement SMS surveys to reach younger demographics and those without regular internet access.
Incentives: Offer gift cards or other small incentives for participation, particularly for online respondents. This can enhance response rates and engagement.

To effectively reach our target population and capture a diverse range of perspectives, we will implement a multi-mode recruitment strategy tailored to different demographic groups and communication preferences. This approach includes online surveys using a Google Forms questionnaire, which will be widely distributed through social media, email lists, and community networks to maximize reach among individuals who frequently engage online. The user-friendly platform allows participants to complete the survey quickly and anonymously on any internet-enabled device. The survey can be accessed through the following link: [https://forms.gle/oSbad52Vuw9Z9Wf46](https://forms.gle/oSbad52Vuw9Z9Wf46). Additionally, we will conduct live telephone interviews to include participants who may not be reachable through online channels, ensuring we capture responses from populations that might otherwise be underrepresented. To further engage younger demographics and individuals with limited internet access, we will implement SMS-based surveys, allowing participants to respond quickly via text. To encourage participation and improve response rates, small incentives such as digital gift cards will be offered, particularly for online respondents, with details communicated at the survey's start and awarded upon completion to ensure transparency. This comprehensive approach will enable us to gather a robust and representative dataset, providing valuable insights into the preferences and priorities of voters across multiple demographics.

## Poll Aggregation and Modeling

Poll results will be aggregated using statistical methods to identify trends and analyze historical voting patterns. For model building, we will implement a weighted analysis to ensure demographic representation, applying specific weights based on factors such as age, gender, race, and education level, as well as state significance to reflect regional variations in voter behavior. Predictive analytics will primarily involve logistic regression to model voting preferences and forecast election outcomes, supplemented by time-series analysis to track changes in voter sentiment over the campaign period. To enhance our findings, we will combine our data with reputable sources like FiveThirtyEight, utilizing their aggregation techniques to enrich our analysis. 

\newpage

# References

